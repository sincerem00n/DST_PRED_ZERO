{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import argparse\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Add these argument for training')\n",
    "parser.add_argument('--dir', default='results', help='directory for saving trianed mode')\n",
    "parser.add_argument('--feature', required= True)\n",
    "\n",
    "parser.add_argument('--lr', default=0.005, help='learning rate')\n",
    "parser.add_argument('--epochs', default=10, help='epoch number')\n",
    "parser.add_argument('--batch_size', default=32)\n",
    "args = parser.parse_args()\n",
    "\n",
    "## Set parameter for the model\n",
    "directory = args.dir                # directory for saving model\n",
    "batch_size = int(args.batch_size)   # batch size\n",
    "learning_rate = float(args.lr)\n",
    "num_epochs = int(args.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>DOY</th>\n",
       "      <th>HR</th>\n",
       "      <th>Scalar_B</th>\n",
       "      <th>BX_GSE_GSM</th>\n",
       "      <th>BY_GSE</th>\n",
       "      <th>BZ_GSE</th>\n",
       "      <th>BY_GSM</th>\n",
       "      <th>BZ_GSM</th>\n",
       "      <th>Proton_Density</th>\n",
       "      <th>SW_Plasma_Temperature</th>\n",
       "      <th>SW_Plasma_Speed</th>\n",
       "      <th>Dst-index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1999</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6.792</td>\n",
       "      <td>-1.686</td>\n",
       "      <td>2.710</td>\n",
       "      <td>5.118</td>\n",
       "      <td>1.284</td>\n",
       "      <td>5.650</td>\n",
       "      <td>6.070</td>\n",
       "      <td>81042.0</td>\n",
       "      <td>415.28</td>\n",
       "      <td>-7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6.884</td>\n",
       "      <td>-4.513</td>\n",
       "      <td>0.390</td>\n",
       "      <td>3.240</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>3.255</td>\n",
       "      <td>7.027</td>\n",
       "      <td>90525.0</td>\n",
       "      <td>400.50</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1999</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7.073</td>\n",
       "      <td>-3.813</td>\n",
       "      <td>-0.826</td>\n",
       "      <td>4.949</td>\n",
       "      <td>-1.808</td>\n",
       "      <td>4.681</td>\n",
       "      <td>6.793</td>\n",
       "      <td>88927.0</td>\n",
       "      <td>405.24</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1999</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6.644</td>\n",
       "      <td>-3.535</td>\n",
       "      <td>1.860</td>\n",
       "      <td>0.244</td>\n",
       "      <td>1.753</td>\n",
       "      <td>0.515</td>\n",
       "      <td>7.129</td>\n",
       "      <td>104190.0</td>\n",
       "      <td>413.07</td>\n",
       "      <td>-8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1999</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6.645</td>\n",
       "      <td>-3.082</td>\n",
       "      <td>1.953</td>\n",
       "      <td>2.664</td>\n",
       "      <td>1.730</td>\n",
       "      <td>2.866</td>\n",
       "      <td>6.963</td>\n",
       "      <td>101510.0</td>\n",
       "      <td>415.57</td>\n",
       "      <td>-8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR  DOY  HR  Scalar_B  BX_GSE_GSM  BY_GSE  BZ_GSE  BY_GSM  BZ_GSM  \\\n",
       "0  1999    1   0     6.792      -1.686   2.710   5.118   1.284   5.650   \n",
       "1  1999    1   1     6.884      -4.513   0.390   3.240  -0.361   3.255   \n",
       "2  1999    1   2     7.073      -3.813  -0.826   4.949  -1.808   4.681   \n",
       "3  1999    1   3     6.644      -3.535   1.860   0.244   1.753   0.515   \n",
       "4  1999    1   4     6.645      -3.082   1.953   2.664   1.730   2.866   \n",
       "\n",
       "   Proton_Density  SW_Plasma_Temperature  SW_Plasma_Speed  Dst-index  \n",
       "0           6.070                81042.0           415.28       -7.0  \n",
       "1           7.027                90525.0           400.50       -4.0  \n",
       "2           6.793                88927.0           405.24       -4.0  \n",
       "3           7.129               104190.0           413.07       -8.0  \n",
       "4           6.963               101510.0           415.57       -8.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/solar_wind_parameters_data_1_hourly_all.csv')\n",
    "\n",
    "data = data.drop(columns=['Unnamed: 0','Timestamp'])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: torch.Size([169651, 24, 9]), torch.Size([169651])\n",
      "Test shape: torch.Size([42413, 24, 9]), torch.Size([42413])\n"
     ]
    }
   ],
   "source": [
    "features = ['Scalar_B', 'BX_GSE_GSM', 'BY_GSE', 'BZ_GSE', 'BY_GSM', 'BZ_GSM', 'Proton_Density', 'SW_Plasma_Temperature', 'SW_Plasma_Speed']\n",
    "target = ['Dst-index']\n",
    "\n",
    "# Select the features and target\n",
    "data = data[features + target]\n",
    "\n",
    "# Normalize the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Split the data into sequences\n",
    "def sequence(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length, :-1]) # all columns except the last\n",
    "        y.append(data[i+seq_length, -1])  # target is the last column\n",
    "        # print(f'X:{np.array(X)}, Y:{np.array(y)}')\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "SEQ_LENGTH = 24\n",
    "X, y = sequence(data_scaled, SEQ_LENGTH)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, shuffle=False)\n",
    "print(f'Train shape: {X_train.shape}, {y_train.shape}')\n",
    "print(f'Test shape: {X_test.shape}, {y_test.shape}')\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_load = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "test_load = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, output_size=1):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        h0 = torch.zeros(1, input_seq.size(1), self.hidden_layer_size).to(self.device)\n",
    "        c0 = torch.zeros(1, input_seq.size(1), self.hidden_layer_size).to(self.device)\n",
    "\n",
    "        lstm_out, _ = self.lstm(input_seq, (h0, c0))\n",
    "\n",
    "        predictions = self.linear(lstm_out)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LSTM(\n",
      "  (lstm): LSTM(9, 64)\n",
      "  (linear): Linear(in_features=64, out_features=1, bias=True)\n",
      ") input_size: 9, hidden_layer_size: 64, output_size: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RAI_65011278/miniconda3/envs/binder_env/lib/python3.9/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 24, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/RAI_65011278/miniconda3/envs/binder_env/lib/python3.9/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([19])) that is different to the input size (torch.Size([19, 24, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/RAI_65011278/miniconda3/envs/binder_env/lib/python3.9/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([13])) that is different to the input size (torch.Size([13, 24, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.0089, Test Loss: 0.0046, Time: 3.30 sec\n",
      "Model improved from 10000000000.0000 to 0.0046 Saving model...\n",
      "Epoch [2/10], Train Loss: 0.0084, Test Loss: 0.0045, Time: 3.30 sec\n",
      "Model improved from 0.0046 to 0.0045 Saving model...\n",
      "Epoch [3/10], Train Loss: 0.0082, Test Loss: 0.0044, Time: 3.30 sec\n",
      "Model improved from 0.0045 to 0.0044 Saving model...\n",
      "Epoch [4/10], Train Loss: 0.0079, Test Loss: 0.0043, Time: 3.28 sec\n",
      "Model improved from 0.0044 to 0.0043 Saving model...\n",
      "Epoch [5/10], Train Loss: 0.0077, Test Loss: 0.0043, Time: 3.28 sec\n",
      "Epoch [6/10], Train Loss: 0.0076, Test Loss: 0.0043, Time: 3.29 sec\n",
      "Model improved from 0.0043 to 0.0043 Saving model...\n",
      "Epoch [7/10], Train Loss: 0.0075, Test Loss: 0.0043, Time: 3.28 sec\n",
      "Model improved from 0.0043 to 0.0043 Saving model...\n",
      "Epoch [8/10], Train Loss: 0.0074, Test Loss: 0.0043, Time: 3.26 sec\n",
      "Model improved from 0.0043 to 0.0043 Saving model...\n",
      "Epoch [9/10], Train Loss: 0.0074, Test Loss: 0.0042, Time: 3.27 sec\n",
      "Model improved from 0.0043 to 0.0042 Saving model...\n",
      "Epoch [10/10], Train Loss: 0.0073, Test Loss: 0.0042, Time: 3.30 sec\n",
      "Model improved from 0.0042 to 0.0042 Saving model...\n",
      "Model saved to results/lstm_1.pt\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    input_size = len(features)\n",
    "    hidden_layer_size = 64\n",
    "    output_size = 1\n",
    "    model = LSTM(input_size, hidden_layer_size, output_size).to(device)\n",
    "    print(f'Model: {model} input_size: {input_size}, hidden_layer_size: {hidden_layer_size}, output_size: {output_size}')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training Loop\n",
    "    train_loss = 0.0\n",
    "    best_loss = 1e10\n",
    "    best_epoch = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        for i, data in enumerate(train_load):\n",
    "            X_train, y_train = data\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_train)\n",
    "\n",
    "            loss = criterion(y_pred, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        end = time.time()\n",
    "            \n",
    "        train_loss = train_loss / len(train_load)\n",
    "\n",
    "        # Test the model\n",
    "        test_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_load):\n",
    "                X_test, y_test = data\n",
    "                y_pred = model(X_test)\n",
    "                loss = criterion(y_pred, y_test)\n",
    "                test_loss += loss.item()\n",
    "        test_loss = test_loss / len(test_load)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Time: {end-start:.2f} sec\")\n",
    "    \n",
    "        if test_loss < best_loss:\n",
    "            print(f\"Model improved from {best_loss:.4f} to {test_loss:.4f} Saving model...\")\n",
    "            best_loss = test_loss\n",
    "            best_epoch = epoch + 1\n",
    "\n",
    "            # Save checkpoint\n",
    "            checkpoint = {\n",
    "                'epoch': best_epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_loss\n",
    "            }\n",
    "            torch.save(checkpoint, f'{directory}/lstm_{best_epoch}.pt')\n",
    "\n",
    "    # Save the model\n",
    "    # save_path = f'{directory}/lstm_{args.feature}.pt'\n",
    "    save_path = f'{directory}/lstm_1.pt'\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    \n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    # torch.save(model.state_dict(), f'{directory}/lstm_{args.feature}.pt')\n",
    "    print(f\"Model saved to {directory}/lstm_1.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python binder_env",
   "language": "python",
   "name": "binder_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
