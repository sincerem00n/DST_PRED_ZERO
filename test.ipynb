{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "from model import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Add these argument for training')\n",
    "parser.add_argument('--dir', default='results', help='directory for saving trianed mode')\n",
    "parser.add_argument('--feature', required= True)\n",
    "\n",
    "parser.add_argument('--lr', default=0.005, help='learning rate')\n",
    "parser.add_argument('--epochs', default=10, help='epoch number')\n",
    "parser.add_argument('--batch_size', default=32)\n",
    "args = parser.parse_args()\n",
    "\n",
    "## Set parameter for the model\n",
    "directory = args.dir                # directory for saving model\n",
    "batch_size = int(args.batch_size)   # batch size\n",
    "learning_rate = float(args.lr)\n",
    "num_epochs = int(args.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/solar_wind_parameters_data_1_hourly_all.csv')\n",
    "\n",
    "data = data.drop(columns=['Unnamed: 0','Timestamp'])\n",
    "\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Normalize the data\\nfrom sklearn.preprocessing import MinMaxScaler\\nfrom sklearn.model_selection import train_test_split\\n\\nscaler = MinMaxScaler()\\ndata_scaled = scaler.fit_transform(data)\\n\\n# Split the data into sequences\\ndef sequence(data, seq_length):\\n    X, y = [], []\\n    for i in range(len(data) - seq_length):\\n        X.append(data[i:i+seq_length, :-1]) # all columns except the last\\n        y.append(data[i+seq_length, -1])  # target is the last column\\n        # print(f'X:{np.array(X)}, Y:{np.array(y)}')\\n    return np.array(X), np.array(y)\\n\\nSEQ_LENGTH = 24\\nX, y = sequence(data_scaled, SEQ_LENGTH)\\n\\n\\n# Convert to PyTorch tensors\\nX_tensor = torch.tensor(X, dtype=torch.float32).to(device)\\ny_tensor = torch.tensor(y, dtype=torch.float32).to(device)\\n\\n# Split the data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, shuffle=False)\\nprint(f'Train shape: {X_train.shape}, {y_train.shape}')\\nprint(f'Test shape: {X_test.shape}, {y_test.shape}')\\n\\ntrain_data = TensorDataset(X_train, y_train)\\ntest_data = TensorDataset(X_test, y_test)\\n\\ntrain_load = DataLoader(train_data, batch_size=batch_size, shuffle=False)\\ntest_load = DataLoader(test_data, batch_size=batch_size, shuffle=False)\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['Scalar_B', 'BX_GSE_GSM', 'BY_GSE', 'BZ_GSE', 'BY_GSM', 'BZ_GSM', 'Proton_Density', 'SW_Plasma_Temperature', 'SW_Plasma_Speed']\n",
    "target = ['Dst-index']\n",
    "\n",
    "# Select the features and target\n",
    "data = data[features + target]\n",
    "# data.head()\n",
    "\n",
    "# Normalize the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Split the data into sequences\n",
    "def sequence(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length, :-1]) # all columns except the last\n",
    "        y.append(data[i+seq_length, -1])  # target is the last column\n",
    "        # print(f'X:{np.array(X)}, Y:{np.array(y)}')\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "SEQ_LENGTH = 24\n",
    "X, y = sequence(data_scaled, SEQ_LENGTH)\n",
    "\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, shuffle=False)\n",
    "print(f'Train shape: {X_train.shape}, {y_train.shape}')\n",
    "print(f'Test shape: {X_test.shape}, {y_test.shape}')\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_load = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "test_load = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from Dst-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scalar_B</th>\n",
       "      <th>BZ_GSE</th>\n",
       "      <th>SW_Plasma_Temperature</th>\n",
       "      <th>SW_Proton_Density</th>\n",
       "      <th>SW_Plasma_Speed</th>\n",
       "      <th>Flow_pressure</th>\n",
       "      <th>E_elecrtric_field</th>\n",
       "      <th>Dst-index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.5</td>\n",
       "      <td>-3.3</td>\n",
       "      <td>9999999</td>\n",
       "      <td>2.0</td>\n",
       "      <td>430</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.03</td>\n",
       "      <td>-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>145000</td>\n",
       "      <td>2.1</td>\n",
       "      <td>439</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.7</td>\n",
       "      <td>1.9</td>\n",
       "      <td>131000</td>\n",
       "      <td>1.9</td>\n",
       "      <td>449</td>\n",
       "      <td>0.77</td>\n",
       "      <td>-1.17</td>\n",
       "      <td>-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>76000</td>\n",
       "      <td>1.6</td>\n",
       "      <td>446</td>\n",
       "      <td>0.64</td>\n",
       "      <td>-1.43</td>\n",
       "      <td>-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>78000</td>\n",
       "      <td>1.4</td>\n",
       "      <td>468</td>\n",
       "      <td>0.61</td>\n",
       "      <td>-1.54</td>\n",
       "      <td>-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Scalar_B  BZ_GSE  SW_Plasma_Temperature  SW_Proton_Density  \\\n",
       "0       6.5    -3.3                9999999                2.0   \n",
       "1       6.3    -1.5                 145000                2.1   \n",
       "2       5.7     1.9                 131000                1.9   \n",
       "3       5.7     2.5                  76000                1.6   \n",
       "4       5.9     2.9                  78000                1.4   \n",
       "\n",
       "   SW_Plasma_Speed  Flow_pressure  E_elecrtric_field  Dst-index  \n",
       "0              430           0.74               1.03        -13  \n",
       "1              439           0.81               0.26        -14  \n",
       "2              449           0.77              -1.17        -17  \n",
       "3              446           0.64              -1.43        -17  \n",
       "4              468           0.61              -1.54        -16  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/solar-wind-data/1h/solar_wind_parameters_data_1_hourly_all.csv')\n",
    "# data = data.drop(columns=['Unnamed: 0','Timestamp'])\n",
    "\n",
    "features = ['Scalar_B',  'BZ_GSE', 'SW_Plasma_Temperature',  'SW_Proton_Density','SW_Plasma_Speed', 'Flow_pressure', 'E_elecrtric_field']\n",
    "target = ['Dst-index']\n",
    "\n",
    "# Select the features and target\n",
    "data = data[features + target]\n",
    "data.head()\n",
    "# print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: torch.Size([370771, 24, 7]), torch.Size([370771])\n",
      "Test shape: torch.Size([92693, 24, 7]), torch.Size([92693])\n"
     ]
    }
   ],
   "source": [
    "# Normalize the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Split the data into sequences\n",
    "def sequence(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length, :-1]) # all columns except the last\n",
    "        y.append(data[i+seq_length, -1])  # target is the last column\n",
    "        # print(f'X:{np.array(X)}, Y:{np.array(y)}')\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "SEQ_LENGTH = 24\n",
    "X, y = sequence(data_scaled, SEQ_LENGTH)\n",
    "\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, shuffle=False)\n",
    "print(f'Train shape: {X_train.shape}, {y_train.shape}')\n",
    "print(f'Test shape: {X_test.shape}, {y_test.shape}')\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_load = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "test_load = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **MAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LSTM(\n",
      "  (lstm): LSTM(7, 48, batch_first=True)\n",
      "  (linear): Linear(in_features=48, out_features=1, bias=True)\n",
      ") input_size: 7, hidden_layer_size: 48, output_size: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RAI_65011278/miniconda3/envs/binder_env/lib/python3.9/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/RAI_65011278/miniconda3/envs/binder_env/lib/python3.9/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([19])) that is different to the input size (torch.Size([19, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.0019, Test Loss: 0.0009, Time: 7.87 sec\n",
      "Model improved from 10000000000.0000 to 0.0009 Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RAI_65011278/miniconda3/envs/binder_env/lib/python3.9/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([21])) that is different to the input size (torch.Size([21, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LSTM' object has no attribute 'input_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 57\u001b[0m\n\u001b[1;32m     49\u001b[0m best_epoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Save checkpoint\u001b[39;00m\n\u001b[1;32m     52\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: best_epoch,\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: best_loss,\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_size\u001b[49m,\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_layer_size\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mhidden_layer_size,\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_size\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39moutput_size\n\u001b[1;32m     60\u001b[0m }\n\u001b[1;32m     61\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(checkpoint, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(best_loss\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10000\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     62\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(checkpoint, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/best_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/binder_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1935\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1934\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1935\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1936\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1937\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LSTM' object has no attribute 'input_size'"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_size = len(features)\n",
    "    hidden_layer_size = 48\n",
    "    output_size = 1\n",
    "    model = LSTM(input_size, hidden_layer_size, output_size).to(device)\n",
    "    print(f'Model: {model} input_size: {input_size}, hidden_layer_size: {hidden_layer_size}, output_size: {output_size}')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training Loop\n",
    "    train_loss = 0.0\n",
    "    best_loss = 1e10\n",
    "    best_epoch = 0\n",
    "    rec = []\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        for i, data in enumerate(train_load):\n",
    "            X_train, y_train = data\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_train)\n",
    "\n",
    "            loss = criterion(y_pred, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        end = time.time()\n",
    "            \n",
    "        train_loss = train_loss / len(train_load)\n",
    "\n",
    "        # Test the model\n",
    "        test_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_load):\n",
    "                X_test, y_test = data\n",
    "                y_pred = model(X_test)\n",
    "                loss = criterion(y_pred, y_test)\n",
    "                test_loss += loss.item()\n",
    "        test_loss = test_loss / len(test_load)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Time: {end-start:.2f} sec\")\n",
    "    \n",
    "        if test_loss < best_loss:\n",
    "            print(f\"Model improved from {best_loss:.4f} to {test_loss:.4f} Saving model...\")\n",
    "            best_loss = test_loss\n",
    "            best_epoch = epoch + 1\n",
    "\n",
    "            # Save checkpoint\n",
    "            checkpoint = {\n",
    "                'epoch': best_epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_loss,\n",
    "                'input_size': model.input_size,\n",
    "                'hidden_layer_size': model.hidden_layer_size,\n",
    "                'output_size': model.output_size\n",
    "            }\n",
    "            torch.save(checkpoint, f'./{directory}/model{round(best_loss*10000)}.pth')\n",
    "            torch.save(checkpoint, f'./{directory}/best_model.pth')\n",
    "\n",
    "        # Record results\n",
    "        rec.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'test_loss': test_loss,\n",
    "        })\n",
    "\n",
    "    # Write results to csv\n",
    "    df = pd.DataFrame(rec)\n",
    "    df.to_csv(f'{directory}/results.csv', index=False)\n",
    "\n",
    "    print(f\"Finihsed training after {num_epochs} epochs. Best loss: {best_loss:.4f} at epoch {best_epoch}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Test the Result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss', 'input_size', 'hidden_layer_size', 'output_size'])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LSTM:\n\tMissing key(s) in state_dict: \"lstm.weight_ih_l1\", \"lstm.weight_hh_l1\", \"lstm.bias_ih_l1\", \"lstm.bias_hh_l1\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m LSTM(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_size\u001b[39m\u001b[38;5;124m'\u001b[39m], checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_layer_size\u001b[39m\u001b[38;5;124m'\u001b[39m], output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load the model weights\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set to evaluation mode\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/binder_env/lib/python3.9/site-packages/torch/nn/modules/module.py:2588\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2580\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2581\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2582\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2583\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2584\u001b[0m             ),\n\u001b[1;32m   2585\u001b[0m         )\n\u001b[1;32m   2587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2588\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2589\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2590\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2591\u001b[0m         )\n\u001b[1;32m   2592\u001b[0m     )\n\u001b[1;32m   2593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LSTM:\n\tMissing key(s) in state_dict: \"lstm.weight_ih_l1\", \"lstm.weight_hh_l1\", \"lstm.bias_ih_l1\", \"lstm.bias_hh_l1\". "
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "path = 'results/best_model.pth'\n",
    "checkpoint = torch.load(f'{path}', map_location=device)\n",
    "print(checkpoint.keys())\n",
    "\n",
    "# Recreate the model\n",
    "model = LSTM(checkpoint['input_size'], checkpoint['hidden_layer_size'], output_size=1).to(device)\n",
    "\n",
    "# Load the model weights\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = next(iter(test_load))  # Get one batch from test dataset\n",
    "X_test = X_test.to(device)\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test).cpu().numpy()  # Convert to NumPy for plotting\n",
    "    # print(f'y_pred:{y_pred.shape} {y_pred}')\n",
    "\n",
    "# Convert true values to NumPy\n",
    "y_test = y_test.cpu().numpy()\n",
    "\n",
    "# Fix the shape by flattening y_pred\n",
    "y_pred = y_pred.reshape(y_pred.shape[0], -1).mean(axis=1)  # Average across time steps if needed\n",
    "print(f'y_pred:{y_pred.shape} {y_pred}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python binder_env",
   "language": "python",
   "name": "binder_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
